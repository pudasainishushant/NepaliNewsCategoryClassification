{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9295fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "news_train = {}\n",
    "news_test = {}\n",
    "news = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "directory = \"/home/shushant/Desktop/nlp_projects/NepaliNewsCategoryClassification/data/train\"\n",
    "for filename1 in os.listdir(directory):\n",
    "    for filename in os.listdir(os.path.join(directory,filename1)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory,filename1, filename), encoding=\"utf8\", errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                news.append(content)\n",
    "                labels.append(filename1)\n",
    "                news_train = dict(zip(news,labels))\n",
    "\n",
    "\n",
    "directory = \"/home/shushant/Desktop/nlp_projects/NepaliNewsCategoryClassification/data/test\"\n",
    "for filename1 in os.listdir(directory):\n",
    "    for filename in os.listdir(os.path.join(directory,filename1)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory,filename1, filename), encoding=\"utf8\", errors='ignore') as f:\n",
    "                content = f.read()\n",
    "                news.append(content)\n",
    "                labels.append(filename1)\n",
    "                news_test = dict(zip(news,labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18aa2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df_train = pd.DataFrame(news_train.items(), columns=['news', 'labels'])\n",
    "news_df_test = pd.DataFrame(news_test.items(), columns=['news', 'labels'])\n",
    "news_df = pd.concat([news_df_train,news_df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9698c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class Preprocessor:\n",
    "    # def __init__(self):\n",
    "        # self.this_dir, self.this_file = os.path.split(__file__)\n",
    "\n",
    "    def clean_text(self,text):\n",
    "        '''\n",
    "        accepts the plain text and makes\n",
    "        use of regex for cleaning the noise\n",
    "        :param: text :type:str\n",
    "        :return:cleaned text :type str\n",
    "        '''\n",
    "        text = text.replace(\"\\n\",\"\")\n",
    "        text = text.replace(\"\\t\",\"\")\n",
    "        text = text.replace(\"\\ufeff\",\"\")\n",
    "        #remove all the special characters\n",
    "        symbols = \"।,;?!—-.\"\n",
    "        remove_symbols = str.maketrans('', '', symbols)\n",
    "        text = text.translate(remove_symbols)\n",
    "        # remove digits\n",
    "        digits = \"०१२३४५६७८९\"\n",
    "        remove_digits = str.maketrans('', '', digits)\n",
    "        text = text.translate(remove_digits)\n",
    "        return text\n",
    "        # try:\n",
    "        #     text = text.encode('ascii', errors='ignore').decode(\"utf-8\")\n",
    "        #     return text\n",
    "        # except:\n",
    "        #     return text\n",
    "    \n",
    "\n",
    "    def sentence_tokenize(self, text):\n",
    "        \"\"\"This function tokenize the sentences\n",
    "        \n",
    "        Arguments:\n",
    "            text {string} -- Sentences you want to tokenize\n",
    "        \n",
    "        Returns:\n",
    "            sentence {list} -- tokenized sentence in list\n",
    "        \"\"\"\n",
    "        sentences = text.strip().split(u\"।\")\n",
    "        sentences = [sentence.translate(str.maketrans('', '', string.punctuation)) for sentence in sentences]\n",
    "        return sentences\n",
    "\n",
    "    def word_tokenize(self, sentence, new_punctuation=[]):\n",
    "        \"\"\"This function tokenize with respect to word\n",
    "        \n",
    "        Arguments:\n",
    "            sentence {string} -- sentence you want to tokenize\n",
    "            new_punctuation {list} -- more punctutaion for tokenizing  default ['।',',',';','?','!','—','-']\n",
    "        \n",
    "        Returns:\n",
    "            list -- tokenized words\n",
    "        \"\"\"\n",
    "        punctuations = ['।', ',', ';', '?', '!', '—', '-', '.']\n",
    "        if new_punctuation:\n",
    "            punctuations = set(punctuations + new_punctuation)\n",
    "\n",
    "        for punct in punctuations:\n",
    "            sentence = ' '.join(sentence.split(punct))\n",
    "\n",
    "        return sentence.split()\n",
    "\n",
    "    def character_tokenize(self, word):\n",
    "        \"\"\" Returns the tokenization in character level.\n",
    "        \n",
    "        Arguments:\n",
    "            word {string} -- word to be tokenized in character level.\n",
    "        \n",
    "        Returns:\n",
    "            [list] -- list of characters\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import icu\n",
    "\n",
    "        except:\n",
    "            print(\"please install PyICU\")\n",
    "        \n",
    "        temp_ = icu.BreakIterator.createCharacterInstance(icu.Locale())\n",
    "        temp_.setText(word)\n",
    "        char = []\n",
    "        i = 0\n",
    "        for j in temp_:\n",
    "            s = word[i:j]\n",
    "            char.append(s)\n",
    "            i = j\n",
    "\n",
    "        return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70492d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nep_preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2d96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['cleaned_news'] = news_df['news'].apply(nep_preprocessor.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6528e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_csv(\"data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7c2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
